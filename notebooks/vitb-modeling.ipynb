{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hb5GAuOB4fcF",
   "metadata": {
    "id": "hb5GAuOB4fcF"
   },
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83246ac9",
   "metadata": {
    "id": "83246ac9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uYmjA0OY4hlV",
   "metadata": {
    "id": "uYmjA0OY4hlV"
   },
   "source": [
    "## Constants\n",
    "\n",
    "References:\n",
    "\n",
    "* https://github.com/google-research/vision_transformer/blob/main/vit_jax/configs/models.py#L103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5770fcd",
   "metadata": {
    "id": "c5770fcd"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 224\n",
    "PATCH_SIZE = 16  # Size of the patches to be extracted from the input images.\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# ViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 768\n",
    "NUM_HEADS = 12\n",
    "NUM_LAYERS = 12\n",
    "MLP_UNITS = [\n",
    "    PROJECTION_DIM * 4,\n",
    "    PROJECTION_DIM,\n",
    "]\n",
    "DROPOUT_RATE = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XXaYwenB4jCZ",
   "metadata": {
    "id": "XXaYwenB4jCZ"
   },
   "source": [
    "## Vision Transformer blocks\n",
    "\n",
    "References:\n",
    "\n",
    "* https://github.com/google-research/vision_transformer/blob/main/vit_jax/models.py\n",
    "* https://keras.io/examples/vision/image_classification_with_vision_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c8e85b",
   "metadata": {
    "id": "f6c8e85b"
   },
   "outputs": [],
   "source": [
    "def position_embedding(\n",
    "    projected_patches,\n",
    "    num_patches=NUM_PATCHES,\n",
    "    projection_dim=PROJECTION_DIM,\n",
    "    classifier=\"token\",\n",
    "):\n",
    "    # Build the positions.\n",
    "    num_patches += 1 if classifier == \"token\" else 0\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "\n",
    "    # Encode the positions with an Embedding layer.\n",
    "    encoded_positions = layers.Embedding(\n",
    "        input_dim=num_patches,\n",
    "        output_dim=projection_dim,\n",
    "        embeddings_initializer=keras.initializers.RandomNormal(stddev=0.02),\n",
    "    )(positions)\n",
    "\n",
    "    # Add encoded positions to the projected patches.\n",
    "    return projected_patches + encoded_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce45e69e",
   "metadata": {
    "id": "ce45e69e"
   },
   "outputs": [],
   "source": [
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    # Iterate over the hidden units and\n",
    "    # add Dense => Dropout.\n",
    "    for idx, units in enumerate(hidden_units):\n",
    "        x = layers.Dense(\n",
    "            units,\n",
    "            activation=tf.nn.gelu if idx == 0 else None,\n",
    "            kernel_initializer=\"glorot_uniform\",\n",
    "            bias_initializer=keras.initializers.RandomNormal(stddev=1e-6),\n",
    "        )(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35352365",
   "metadata": {
    "id": "35352365"
   },
   "outputs": [],
   "source": [
    "def transformer(encoded_patches):\n",
    "    # Layer normalization 1.\n",
    "    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
    "\n",
    "    # Multi Head Self Attention layer 1.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=DROPOUT_RATE\n",
    "    )(x1, x1)\n",
    "    attention_output = layers.Dropout(DROPOUT_RATE)(attention_output)\n",
    "\n",
    "    # Skip connection 1.\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "    # Layer normalization 2.\n",
    "    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n",
    "\n",
    "    # MLP layer 1.\n",
    "    x4 = mlp(x3, hidden_units=MLP_UNITS, dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "    # Skip connection 2.\n",
    "    encoded_patches = layers.Add()([x2, x4])\n",
    "\n",
    "    return encoded_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd92458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTClassifier(keras.Model):\n",
    "    def __init__(self, classifier=\"token\", num_classes=10):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.projection = layers.Conv2D(\n",
    "            filters=PROJECTION_DIM,\n",
    "            kernel_size=(PATCH_SIZE, PATCH_SIZE),\n",
    "            strides=(PATCH_SIZE, PATCH_SIZE),\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "\n",
    "        if self.classifier == \"token\":\n",
    "            initial_value = tf.zeros((1, 1, PROJECTION_DIM))\n",
    "            self.cls_token = tf.Variable(\n",
    "                initial_value=initial_value, trainable=True, name=\"cls\"\n",
    "            )\n",
    "\n",
    "        if self.classifier == \"gap\":\n",
    "            self.gap_layer = layers.GlobalAvgPool1D()\n",
    "\n",
    "        self.dropout = layers.Dropout(DROPOUT_RATE)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "        self.classifier_head = layers.Dense(num_classes, kernel_initializer=\"zeros\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Create patches and project the pathces.\n",
    "        projected_patches = self.projection(inputs)\n",
    "        n, h, w, c = projected_patches.shape\n",
    "        projected_patches = tf.reshape(projected_patches, [n, h * w, c])\n",
    "\n",
    "        # Append class token if needed.\n",
    "        if self.classifier == \"token\":\n",
    "            cls_token = tf.tile(self.cls_token, (n, 1, 1))\n",
    "            projected_patches = tf.concat([cls_token, projected_patches], axis=1)\n",
    "\n",
    "        # Add positional embeddings to the projected patches.\n",
    "        encoded_patches = position_embedding(\n",
    "            projected_patches, classifier=self.classifier\n",
    "        )  # (B, number_patches, projection_dim)\n",
    "        encoded_patches = self.dropout(encoded_patches)\n",
    "\n",
    "        # Iterate over the number of layers and stack up blocks of\n",
    "        # Transformer.\n",
    "        for i in range(NUM_LAYERS):\n",
    "            # Add a Transformer block.\n",
    "            encoded_patches = transformer(encoded_patches)\n",
    "\n",
    "        # Final layer normalization.\n",
    "        representation = self.layer_norm(encoded_patches)\n",
    "\n",
    "        # Pool representation.\n",
    "        if self.classifier == \"token\":\n",
    "            encoded_patches = representation[:, 0]\n",
    "        elif self.classifier == \"gap\":\n",
    "            encoded_patches = self.gap_layer(representation)\n",
    "\n",
    "        # Classification head.\n",
    "        output = self.classifier_head(encoded_patches)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49465a1c",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9467578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 13:10:28.603093: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier = ViTClassifier()\n",
    "random_logits = vit_classifier(tf.ones((10, 224, 224, 3)))\n",
    "random_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd47f3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier_w_gap = ViTClassifier(classifier=\"gap\")\n",
    "vit_classifier_w_gap(tf.ones((10, 224, 224, 3))).shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "data2vec-vision.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
